import logging
import uuid
from typing import Any, Dict
from uuid import UUID

from app.agent.executors.base import BaseNodeExecutor, ValidationResult
from app.db.postgres.models.rag import PipelineJob, PipelineJobStatus, ExecutablePipeline, PipelineStepStatus
from app.rag.pipeline.executor import PipelineExecutor
from sqlalchemy import select

logger = logging.getLogger(__name__)

class RAGNodeExecutor(BaseNodeExecutor):
    async def validate_config(self, config: Dict[str, Any]) -> ValidationResult:
        if not config.get("pipeline_id"):
             return ValidationResult(valid=False, errors=["Missing 'pipeline_id' in configuration"])
        return ValidationResult(valid=True)

    async def execute(self, state: Dict[str, Any], config: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Execute a RAG Pipeline.
        1. Create PipelineJob.
        2. Execute via PipelineExecutor.
        3. Retrieve output.
        """
        pipeline_id_str = config.get("pipeline_id")
        executable_pipeline_id_str = config.get("executable_pipeline_id", pipeline_id_str) # Support both
        
        if not executable_pipeline_id_str:
            raise ValueError("Missing executable_pipeline_id")

        executable_pipeline_id = UUID(executable_pipeline_id_str)
        
        # Resolve Input
        # Assume input comes from 'messages' (last user message) or 'context'
        query = ""
        messages = state.get("messages", [])
        if messages:
            last_msg = messages[-1]
            query = getattr(last_msg, "content", str(last_msg))

        input_params = {"query": query, "top_k": config.get("top_k", 5)}

        # 1. Create Job
        job = PipelineJob(
            id=uuid.uuid4(),
            tenant_id=self.tenant_id,
            executable_pipeline_id=executable_pipeline_id,
            status=PipelineJobStatus.PENDING,
            input_params=input_params,
            triggered_by="agent_execution"
        )
        self.db.add(job)
        await self.db.commit()
        await self.db.refresh(job) # Get ID if autogenerated, but we generated it

        # 2. Execute
        executor = PipelineExecutor(self.db)
        try:
            # execute_job is likely designed to be awaited
            await executor.execute_job(job.id)
            
            # 3. Fetch Result
            # Re-fetch job to get latest state
            await self.db.refresh(job)
            
            if job.status == PipelineJobStatus.FAILED:
                raise Exception(f"RAG Pipeline failed: {job.error_message}")
            
            # Find the final output steps
            # Ideally we check the last step's output
            # For now, let's grab all step executions and find the last one
            from app.db.postgres.models.rag import PipelineStepExecution
            stmt = select(PipelineStepExecution).where(PipelineStepExecution.job_id == job.id).order_by(PipelineStepExecution.execution_order.desc())
            steps = (await self.db.execute(stmt)).scalars().all()
            
            output_context = {}
            if steps:
                final_step = steps[0] # Last step because of desc order
                output_context = final_step.output_data or {}

            return {
                "rag_output": output_context,
                "context": output_context # Merge into context
            }

        except Exception as e:
            logger.error(f"RAG Job failed: {e}")
            raise e
