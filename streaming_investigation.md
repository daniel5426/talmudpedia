# Investigation Report: Token Streaming Buffering

## Problem Statement
Tokens generated by the AI agent appear in the Playground and Agent Builder all at once rather than streaming in real-time, despite the backend implementation using asynchronous generators and `StreamingResponse`.

## Testing & Methodology
To isolate the cause, we implemented several layers of tracing and preventative measures:

1.  **Backend Trace**: Mixed `[DEBUG]` logs into the Python event loop and the FastAPI `event_generator`.
2.  **Frontend Trace**: Added `RAW CHUNK` logging in the browser (via `useAgentRunController.ts`) to see the exact byte count and timing of packets received by the `ReadableStream`.
3.  **Buffer Breaking**: Added 4KB of stream padding (SSE comments) at the start of the response to try and force intermediate proxies to flush.
4.  **Header Optimization**: Enforced `X-Accel-Buffering: no`, `Cache-Control: no-cache`, and `Content-Encoding: identity`.

## Findings

### 1. Backend behaves correctly
Terminal logs confirm that `[DEBUG] Agents Router: Yielding SSE event: token` occurs sequentially as the LLM generates them. There is no significant internal batching in the Python code.

### 2. Network Buffering Confirmed
The browser logs revealed the bottleneck:
- **Chunk 1**: ~255 bytes (Initial handshake/Run ID) received at `6:51:42 PM`.
- **Chunk 2**: **5,897 bytes** received at `6:51:48 PM`.

This 5.8KB chunk contained **multiple** `token` events. Because the browser didn't receive the data until the buffer reached this size (or the 6-second timeout occurred), React only updated the UI once at the very end.

## Root Cause
The buffering is happening at the **Network/Proxy Layer**.
- **Next.js Dev Proxy**: The `rewrites` in `next.config.ts` act as a proxy between the frontend (port 3000) and backend (port 8000). Next.js's underlying proxy (often `http-proxy`) frequently buffers responses, especially in development mode.
- **Middleware**: Any intermediate layer (Nginx, Gzip, or corporate proxies) that expects a certain payload size before flushing.

## Recommended Next Steps
1.  **Bypass the Proxy**: Update the frontend `agentService` to call the backend directly (e.g., `http://127.0.0.1:8000`) for the stream endpoint, bypassing the Next.js `/api/py` rewrite.
2.  **Increase Padding**: Some proxies require up to 8KB or 16KB of data to trigger an unbuffered "fast-path."
3.  **HTTP/2**: If possible, enforce HTTP/2 which handles streaming more natively than HTTP/1.1 with chunked encoding.
